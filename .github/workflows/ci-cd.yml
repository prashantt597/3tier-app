name: CI/CD Pipeline
on:
  push:
    branches: [ main ]
  workflow_dispatch:
jobs:
  debug:
    runs-on: ubuntu-latest
    steps:
      - name: Debug trigger
        run: echo "Workflow triggered successfully"

  setup:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

  test:
    needs: setup
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          cd client
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip list

      - name: Check for dependency vulnerabilities
        run: |
          cd client
          pip install pip-audit
          pip-audit -r requirements.txt || true

      - name: Run unit tests with coverage
        run: |
          cd client
          python -m pytest -vv --cov=app --cov-report=xml tests/ || echo "Test errors: $(cat pytest.log)"

  scan:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Shallow clones should be disabled for better relevancy of analysis

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and push Docker image
        run: |
          cd client
          docker build -t ${{ secrets.DOCKERHUB_USERNAME }}/git_action_aws:latest .
          docker push ${{ secrets.DOCKERHUB_USERNAME }}/git_action_aws:latest

      - name: Verify Docker image availability
        run: |
          docker pull ${{ secrets.DOCKERHUB_USERNAME }}/git_action_aws:latest || { echo "Failed to pull image"; exit 1; }

      - name: Run Trivy scan
        uses: aquasecurity/trivy-action@master
        env:
          TRIVY_DISABLE_VEX_NOTICE: true
        with:
          image-ref: 'docker.io/${{ secrets.DOCKERHUB_USERNAME }}/git_action_aws:latest'
          format: 'table'
          exit-code: '1'
          ignore-unfixed: true
          vuln-type: 'os,library'
          severity: 'CRITICAL,HIGH'
          skip-version-check: true

      - name: Debug SonarCloud configuration
        run: |
          echo "SONAR_HOST_URL: https://sonarcloud.io"
          echo "SONAR_TOKEN is set: ${{ secrets.SONAR_TOKEN != '' }}"

      - name: SonarCloud Scan
        uses: SonarSource/sonarqube-scan-action@v5
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: https://sonarcloud.io
        with:
          projectBaseDir: client
          args: >
            -Dsonar.projectKey=${{ github.repository }}
            -Dsonar.organization=${{ secrets.SONAR_ORGANIZATION }} # Add this if you have a SonarCloud organization
            -Dsonar.sources=.
            -Dsonar.exclusions=tests/**,*/migrations/*
            -Dsonar.python.coverage.reportPaths=coverage.xml

      - name: Debug Docker container
        run: |
          docker run -d -p 8000:8000 --name test-container -e SECRET_KEY=${{ secrets.SECRET_KEY }} ${{ secrets.DOCKERHUB_USERNAME }}/git_action_aws:latest
          sleep 20
          docker ps -a
          docker logs test-container || true
          netstat -tuln | grep 8000 || true

      - name: Verify Docker image health
        continue-on-error: true
        run: |
          docker run -d -p 8000:8000 --name test-container -e SECRET_KEY=${{ secrets.SECRET_KEY }} ${{ secrets.DOCKERHUB_USERNAME }}/git_action_aws:latest
          sleep 20
          for i in {1..3}; do
            curl -f http://localhost:8000/health && exit 0
            echo "Retry $i: Health check failed, retrying..."
            sleep 5
          done
          echo "Health check failed after 3 retries"
          docker logs test-container
          exit 1

  deploy:
    needs: scan
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-south-1

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.7

      - name: Debug Terraform configuration
        run: |
          cd infra
          terraform version
          terraform init -reconfigure
          terraform providers
          terraform plan -out=tfplan
          terraform show tfplan
          terraform output -json > terraform_outputs.json

      - name: Terraform Apply
        continue-on-error: true
        run: |
          cd infra
          terraform apply -auto-approve

      - name: Wait for and Recreate EKS Cluster if Needed
        continue-on-error: true
        run: |
          echo "Waiting for or recreating EKS cluster..."
          for i in {1..60}; do
            STATUS=$(aws eks --region ap-south-1 describe-cluster --name git-action-eks --query 'cluster.status' --output text 2>/dev/null || echo "NOT_FOUND")
            if [ "$STATUS" = "ACTIVE" ]; then
              echo "Cluster is ACTIVE"
              exit 0
            elif [ "$STATUS" = "NOT_FOUND" ]; then
              echo "Cluster not found, triggering recreation ($i/60)..."
              cd infra
              terraform apply -auto-approve
              cd ..
            else
              echo "Cluster status: $STATUS, retrying ($i/60)..."
            fi
            sleep 30
          done
          echo "Cluster did not become ACTIVE after 30 minutes"
          exit 1

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Debug kubectl configuration
        continue-on-error: true
        run: |
          aws eks --region ap-south-1 describe-cluster --name git-action-eks --query 'cluster.{endpoint:endpoint, status:status, vpcId:vpcId, subnetIds:subnetIds, securityGroups:securityGroups}' --output json || true
          aws eks --region ap-south-1 update-kubeconfig --name git-action-eks
          kubectl config view
          kubectl cluster-info || true
          kubectl get nodes -o wide || true
          echo "Testing EKS endpoint connectivity"
          EKS_ENDPOINT=$(aws eks describe-cluster --region ap-south-1 --name git-action-eks --query 'cluster.endpoint' --output text)
          curl -k -v $EKS_ENDPOINT || true
          ping -c 3 $EKS_ENDPOINT || true

      - name: Configure kubectl
        run: |
          aws eks --region ap-south-1 update-kubeconfig --name git-action-eks

      - name: Create Kubernetes Secret
        continue-on-error: true
        run: |
          echo "SECRET_KEY is set: ${{ secrets.SECRET_KEY != '' }}"
          if [ -z "${{ secrets.SECRET_KEY }}" ]; then
            echo "Error: SECRET_KEY is empty or unset"
            exit 1
          fi
          kubectl create secret generic app-secrets --from-literal=secret-key=${{ secrets.SECRET_KEY }} -n default --dry-run=client -o yaml | kubectl apply -f --request-timeout=60s

      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.0'

      - name: Install AWS Load Balancer Controller
        continue-on-error: true
        run: |
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          ALB_CONTROLLER_ROLE_ARN=$(cat ../infra/terraform_outputs.json | jq -r '.aws_load_balancer_controller_role_arn.value')
          helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
            --namespace kube-system \
            --set clusterName=git-action-eks \
            --set serviceAccount.create=true \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"=$ALB_CONTROLLER_ROLE_ARN \
            --wait --timeout 10m

      - name: Deploy to EKS with Helm
        continue-on-error: true
        run: |
          helm package helm/fastapi-app
          mv fastapi-app-*.tgz fastapi-app.tgz
          helm upgrade --install fastapi-app ./fastapi-app.tgz --namespace default --set image.tag=latest --timeout 15m

      - name: Debug Ingress and ALB
        continue-on-error: true
        run: |
          echo "Listing all ingresses:"
          kubectl get ingress -n default -o yaml || true
          echo "Describing ingress (if any):"
          kubectl describe ingress -n default || true
          echo "Checking ALB controller pods:"
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -o wide || true

      - name: Get ALB URL
        id: alb_url
        continue-on-error: true
        run: |
          for i in {1..15}; do
            ALB_URL=$(kubectl get ingress -n default -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}' --request-timeout=60s || true)
            if [ -n "$ALB_URL" ]; then
              echo "ALB_URL=$ALB_URL" >> $GITHUB_ENV
              echo "ALB URL: $ALB_URL"
              exit 0
            fi
            echo "Retrying ALB URL retrieval ($i/15)..."
            sleep 30
          done
          echo "Failed to retrieve ALB URL after 15 attempts"
          exit 1

      - name: Output ALB URL
        run: |
          echo "Application is accessible at: ${{ env.ALB_URL }}"