name: CI/CD Pipeline
on:
  push:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      destroy:
        description: 'Trigger Terraform destroy'
        required: false
        default: 'false'
jobs:
  debug:
    runs-on: ubuntu-latest
    steps:
      - name: Debug trigger
        run: echo "Workflow triggered successfully"

  setup:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.7
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-south-1

  test:
    needs: setup
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          cd client
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip list
      - name: Check for dependency vulnerabilities
        run: |
          cd client
          pip install pip-audit
          pip-audit -r requirements.txt || true
      - name: Run unit tests with coverage
        run: |
          cd client
          python -m pytest -vv --cov=app --cov-report=xml tests/ || echo "Test errors: $(cat pytest.log)"

  scan:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Shallow clones should be disabled for better relevancy of analysis
      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      - name: Check and Create Docker Repository
        run: |
          REPO="${{ secrets.DOCKERHUB_USERNAME }}/3tier-app"
          if ! curl -s -f -u ${{ secrets.DOCKERHUB_USERNAME }}:${{ secrets.DOCKERHUB_TOKEN }} https://hub.docker.com/v2/repositories/$REPO/ > /dev/null 2>&1; then
            curl -X POST -u ${{ secrets.DOCKERHUB_USERNAME }}:${{ secrets.DOCKERHUB_TOKEN }} https://hub.docker.com/v2/repositories/ -d "{\"namespace\":\"${{ secrets.DOCKERHUB_USERNAME }}\",\"name\":\"3tier-app\",\"description\":\"Automated 3tier-app repository\"}"
            echo "Created Docker repository: $REPO"
          else
            echo "Docker repository $REPO already exists"
          fi
      - name: Build and push Docker image
        run: |
          cd client
          docker build -t ${{ secrets.DOCKERHUB_USERNAME }}/3tier-app:${{ github.sha }} .
          docker push ${{ secrets.DOCKERHUB_USERNAME }}/3tier-app:${{ github.sha }}
      - name: Verify Docker image availability
        run: |
          docker pull ${{ secrets.DOCKERHUB_USERNAME }}/3tier-app:${{ github.sha }} || { echo "Failed to pull image"; exit 1; }
      - name: Run Trivy scan
        uses: aquasecurity/trivy-action@master
        env:
          TRIVY_DISABLE_VEX_NOTICE: true
        with:
          image-ref: 'docker.io/${{ secrets.DOCKERHUB_USERNAME }}/3tier-app:${{ github.sha }}'
          format: 'table'
          exit-code: '1'
          ignore-unfixed: true
          vuln-type: 'os,library'
          severity: 'CRITICAL,HIGH'
          skip-version-check: true
      - name: Debug SonarCloud configuration
        run: |
          echo "SONAR_HOST_URL: https://sonarcloud.io"
          echo "SONAR_TOKEN is set: ${{ secrets.SONAR_TOKEN != '' }}"
      - name: SonarCloud Scan
        uses: SonarSource/sonarqube-scan-action@v5
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: https://sonarcloud.io
        with:
          projectBaseDir: client
          args: >
            -Dsonar.projectKey=prashantt597_3tier-app
            -Dsonar.organization=${{ secrets.SONAR_ORGANIZATION }} # Add this if you have a SonarCloud organization
            -Dsonar.sources=.
            -Dsonar.exclusions=tests/**,*/migrations/*
            -Dsonar.python.coverage.reportPaths=coverage.xml
      - name: Debug Docker container
        run: |
          docker run -d -p 8000:8000 --name test-container -e SECRET_KEY=${{ secrets.SECRET_KEY }} ${{ secrets.DOCKERHUB_USERNAME }}/3tier-app:${{ github.sha }}
          sleep 20
          docker ps -a
          docker logs test-container || true
          netstat -tuln | grep 8000 || true
      - name: Verify Docker image health
        continue-on-error: true
        run: |
          docker run -d -p 8000:8000 --name test-container -e SECRET_KEY=${{ secrets.SECRET_KEY }} ${{ secrets.DOCKERHUB_USERNAME }}/3tier-app:${{ github.sha }}
          sleep 20
          for i in {1..3}; do
            curl -f http://localhost:8000/health && exit 0
            echo "Retry $i: Health check failed, retrying..."
            sleep 5
          done
          echo "Health check failed after 3 retries"
          docker logs test-container
          exit 1

  deploy:
    needs: scan
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-south-1
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.7
      - name: Terraform Init (Local Backend)
        run: |
          cd infra
          terraform init
      - name: Terraform Apply S3 Resources
        run: |
          cd infra
          echo "Applying with account_id: ${{ secrets.AWS_ACCOUNT_ID }}"
          terraform apply -target=random_string.s3_prefix -target=aws_s3_bucket.terraform_state -target=aws_s3_bucket_versioning.terraform_state -target=aws_s3_bucket_public_access_block.terraform_state -target=aws_dynamodb_table.terraform_locks -auto-approve -var="region=ap-south-1" -var="account_id=${{ secrets.AWS_ACCOUNT_ID }}" -var="image_repository=docker.io/${{ secrets.DOCKERHUB_USERNAME }}/3tier-app" -var="image_tag=latest" -var="deployment_id=${{ github.run_id }}"
      - name: Terraform Init with S3 Backend
        run: |
          cd infra
          terraform init -backend-config="bucket=terraform-state-${{ steps.get_prefix.outputs.prefix }}-${{ secrets.AWS_ACCOUNT_ID }}" \
                         -backend-config="key=terraform.tfstate" \
                         -backend-config="region=ap-south-1" \
                         -backend-config="dynamodb_table=terraform-locks-${{ steps.get_prefix.outputs.prefix }}-${{ secrets.AWS_ACCOUNT_ID }}"
        id: init_with_backend
      - name: Get Random Prefix
        id: get_prefix
        run: |
          cd infra
          echo "Checking Terraform state..."
          terraform state list || echo "State list failed"
          echo "Refreshing Terraform state..."
          terraform refresh -var="region=ap-south-1" -var="account_id=${{ secrets.AWS_ACCOUNT_ID }}" -var="image_repository=docker.io/${{ secrets.DOCKERHUB_USERNAME }}/3tier-app" -var="image_tag=latest" -var="deployment_id=${{ github.run_id }}" || echo "Refresh failed"
          echo "Capturing prefix output..."
          (terraform show -json | jq -r '.values.outputs.s3_prefix.value' > prefix.txt) 2>/dev/null || echo "Output not found" > prefix.txt
          PREFIX=$(cat prefix.txt)
          echo "prefix=$PREFIX" >> $GITHUB_OUTPUT
          echo "Generated prefix: $PREFIX"
          rm -f prefix.txt
      - name: Check VPC Limit
        run: |
          cd infra
          VPC_COUNT=$(aws ec2 describe-vpcs --region ap-south-1 --query 'Vpcs[*].VpcId' --output text | wc -w)
          echo "Current VPC count in ap-south-1: $VPC_COUNT"
          if [ "$VPC_COUNT" -ge 5 ]; then
            echo "Error: VPC limit (5) exceeded in ap-south-1. Please delete unused VPCs or request a quota increase via AWS Support at https://console.aws.amazon.com/support/home#/case/create."
            exit 1
          fi
      - name: Check Existing Resources
        run: |
          cd infra
          DEPLOYMENT_ID="${{ github.run_id }}"
          EXISTING_VPCS=$(aws ec2 describe-vpcs --region ap-south-1 --filters "Name=tag:DeploymentID,Values=$DEPLOYMENT_ID" --query 'Vpcs[*].VpcId' --output text)
          EXISTING_S3=$(aws s3 ls s3://terraform-state-*${{ secrets.AWS_ACCOUNT_ID }} --region ap-south-1 --query 'Buckets[?contains(Name, `terraform-state-`) && contains(Name, `${{ secrets.AWS_ACCOUNT_ID }}`) && !contains(Name, `terraform-state-${{ steps.get_prefix.outputs.prefix }}`)].Name' --output text)
          if [ -n "$EXISTING_VPCS" ]; then
            echo "Found existing VPCs from this deployment: $EXISTING_VPCS"
            aws ec2 delete-vpc --vpc-id $EXISTING_VPCS --region ap-south-1 || echo "Failed to delete existing VPCs"
          fi
          if [ -n "$EXISTING_S3" ]; then
            echo "Found existing S3 buckets from this deployment: $EXISTING_S3"
            for bucket in $EXISTING_S3; do
              aws s3 rb s3://$bucket --force --region ap-south-1 || echo "Failed to delete bucket $bucket"
            done
          fi
      - name: Debug Terraform configuration
        run: |
          cd infra
          terraform version
          terraform init -reconfigure
          terraform providers
          terraform plan -out=tfplan -var="vpc_cidr=10.0.0.0/16" -var="cluster_name=3tier-eks" -var="node_count=2" -var="region=ap-south-1" -var="account_id=${{ secrets.AWS_ACCOUNT_ID }}" -var="image_repository=docker.io/${{ secrets.DOCKERHUB_USERNAME }}/3tier-app" -var="image_tag=${{ github.sha }}" -var="deployment_id=${{ github.run_id }}"
          terraform show tfplan
          terraform output -json > terraform_outputs.json
      - name: Terraform Apply
        if: github.event.inputs.destroy != 'true' && success()
        run: |
          cd infra
          terraform apply -auto-approve -var="vpc_cidr=10.0.0.0/16" -var="cluster_name=3tier-eks" -var="node_count=2" -var="region=ap-south-1" -var="account_id=${{ secrets.AWS_ACCOUNT_ID }}" -var="image_repository=docker.io/${{ secrets.DOCKERHUB_USERNAME }}/3tier-app" -var="image_tag=${{ github.sha }}" -var="deployment_id=${{ github.run_id }}"
        continue-on-error: true
      - name: Cleanup on Failure
        if: github.event.inputs.destroy != 'true' && failure()
        run: |
          cd infra
          DEPLOYMENT_ID="${{ github.run_id }}"
          echo "Cleaning up resources for failed deployment $DEPLOYMENT_ID"
          EXISTING_VPCS=$(aws ec2 describe-vpcs --region ap-south-1 --filters "Name=tag:DeploymentID,Values=$DEPLOYMENT_ID" --query 'Vpcs[*].VpcId' --output text)
          EXISTING_S3=$(aws s3 ls s3://terraform-state-*${{ secrets.AWS_ACCOUNT_ID }} --region ap-south-1 --query 'Buckets[?contains(Name, `terraform-state-`) && contains(Name, `${{ secrets.AWS_ACCOUNT_ID }}`) && !contains(Name, `terraform-state-${{ steps.get_prefix.outputs.prefix }}`)].Name' --output text)
          if [ -n "$EXISTING_VPCS" ]; then
            echo "Deleting VPCs: $EXISTING_VPCS"
            aws ec2 delete-vpc --vpc-id $EXISTING_VPCS --region ap-south-1 || echo "Failed to delete some VPCs"
          fi
          if [ -n "$EXISTING_S3" ]; then
            echo "Deleting S3 buckets: $EXISTING_S3"
            for bucket in $EXISTING_S3; do
              aws s3 rb s3://$bucket --force --region ap-south-1 || echo "Failed to delete bucket $bucket"
            done
          fi
          # Validate and fix bucket name for terraform destroy
          PREFIX="${{ steps.get_prefix.outputs.prefix }}"
          BUCKET="terraform-state-${PREFIX}-${{ secrets.AWS_ACCOUNT_ID }}"
          if aws s3 ls s3://$BUCKET --region ap-south-1 2>&1 | grep -q "NoSuchBucket"; then
            echo "Bucket $BUCKET not found, initializing new state"
            terraform init -force-copy -backend-config="bucket=$BUCKET" -backend-config="key=terraform.tfstate" -backend-config="region=ap-south-1" -backend-config="dynamodb_table=terraform-locks-${PREFIX}-${{ secrets.AWS_ACCOUNT_ID }}" || echo "Terraform init failed, proceeding with manual cleanup"
          else
            echo "Validating Terraform state backend with bucket: $BUCKET"
            terraform init -backend-config="bucket=$BUCKET" -backend-config="key=terraform.tfstate" -backend-config="region=ap-south-1" -backend-config="dynamodb_table=terraform-locks-${PREFIX}-${{ secrets.AWS_ACCOUNT_ID }}" || echo "Terraform init failed, proceeding with manual cleanup"
          fi
          terraform destroy -auto-approve -var="region=ap-south-1" -var="account_id=${{ secrets.AWS_ACCOUNT_ID }}" -var="image_repository=docker.io/${{ secrets.DOCKERHUB_USERNAME }}/3tier-app" -var="image_tag=${{ github.sha }}" -var="deployment_id=$DEPLOYMENT_ID" || echo "Terraform destroy failed, attempting manual cleanup"
          # Manual cleanup fallback
          terraform state list | grep -E 'aws_vpc|aws_s3_bucket' | while read -r resource; do
            terraform state rm $resource || echo "Failed to remove $resource from state"
            aws_resource=$(echo $resource | sed 's/\..*//')
            if [ "$aws_resource" == "aws_vpc" ]; then
              VPC_ID=$(terraform state show $resource | grep id | awk '{print $3}' || echo "")
              [ -n "$VPC_ID" ] && aws ec2 delete-vpc --vpc-id $VPC_ID --region ap-south-1 || echo "Failed to delete VPC $VPC_ID"
            elif [ "$aws_resource" == "aws_s3_bucket" ]; then
              BUCKET_NAME=$(terraform state show $resource | grep bucket | awk '{print $3}' || echo "")
              [ -n "$BUCKET_NAME" ] && aws s3 rb s3://$BUCKET_NAME --force --region ap-south-1 || echo "Failed to delete bucket $BUCKET_NAME"
            fi
          done
      - name: Validate Cleanup
        if: github.event.inputs.destroy != 'true' && failure()
        run: |
          cd infra
          DEPLOYMENT_ID="${{ github.run_id }}"
          REMAINING_VPCS=$(aws ec2 describe-vpcs --region ap-south-1 --filters "Name=tag:DeploymentID,Values=$DEPLOYMENT_ID" --query 'Vpcs[*].VpcId' --output text)
          REMAINING_S3=$(aws s3 ls s3://terraform-state-*${{ secrets.AWS_ACCOUNT_ID }} --region ap-south-1 --query 'Buckets[?contains(Name, `terraform-state-`) && contains(Name, `${{ secrets.AWS_ACCOUNT_ID }}`) && !contains(Name, `terraform-state-${{ steps.get_prefix.outputs.prefix }}`)].Name' --output text)
          if [ -z "$REMAINING_VPCS" ] && [ -z "$REMAINING_S3" ]; then
            echo "Cleanup successful: No remaining VPCs or S3 buckets for deployment $DEPLOYMENT_ID"
          else
            echo "Warning: Cleanup incomplete - Remaining VPCs: $REMAINING_VPCS, Remaining S3 buckets: $REMAINING_S3"
            exit 1
          fi
      - name: Terraform Destroy
        if: github.event.inputs.destroy == 'true'
        run: |
          cd infra
          terraform destroy -auto-approve -var="vpc_cidr=10.0.0.0/16" -var="cluster_name=3tier-eks" -var="node_count=2" -var="region=ap-south-1" -var="account_id=${{ secrets.AWS_ACCOUNT_ID }}" -var="image_repository=docker.io/${{ secrets.DOCKERHUB_USERNAME }}/3tier-app" -var="image_tag=${{ github.sha }}" -var="deployment_id=${{ github.run_id }}"
      - name: Wait for and Recreate EKS Cluster if Needed
        if: github.event.inputs.destroy != 'true' && success()
        continue-on-error: true
        run: |
          echo "Waiting for or recreating EKS cluster..."
          for i in {1..60}; do
            STATUS=$(aws eks --region ap-south-1 describe-cluster --name 3tier-eks --query 'cluster.status' --output text 2>/dev/null || echo "NOT_FOUND")
            if [ "$STATUS" = "ACTIVE" ]; then
              echo "Cluster is ACTIVE"
              exit 0
            elif [ "$STATUS" = "NOT_FOUND" ]; then
              echo "Cluster not found, triggering recreation ($i/60)..."
              cd infra
              terraform apply -auto-approve -var="vpc_cidr=10.0.0.0/16" -var="cluster_name=3tier-eks" -var="node_count=2" -var="region=ap-south-1" -var="account_id=${{ secrets.AWS_ACCOUNT_ID }}" -var="image_repository=docker.io/${{ secrets.DOCKERHUB_USERNAME }}/3tier-app" -var="image_tag=${{ github.sha }}" -var="deployment_id=${{ github.run_id }}"
              cd ..
            else
              echo "Cluster status: $STATUS, retrying ($i/60)..."
            fi
            sleep 30
          done
          echo "Cluster did not become ACTIVE after 30 minutes"
          exit 1
      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'
      - name: Debug kubectl configuration
        continue-on-error: true
        run: |
          aws eks --region ap-south-1 describe-cluster --name 3tier-eks --query 'cluster.{endpoint:endpoint, status:status, vpcId:vpcId, subnetIds:subnetIds, securityGroups:securityGroups}' --output json || true
          aws eks --region ap-south-1 update-kubeconfig --name 3tier-eks
          kubectl config view
          kubectl cluster-info || true
          kubectl get nodes -o wide || true
          echo "Testing EKS endpoint connectivity"
          EKS_ENDPOINT=$(aws eks describe-cluster --region ap-south-1 --name 3tier-eks --query 'cluster.endpoint' --output text)
          curl -k -v $EKS_ENDPOINT || true
          ping -c 3 $EKS_ENDPOINT || true
      - name: Configure kubectl
        run: |
          aws eks --region ap-south-1 update-kubeconfig --name 3tier-eks
      - name: Create Kubernetes Secret
        continue-on-error: true
        run: |
          echo "SECRET_KEY is set: ${{ secrets.SECRET_KEY != '' }}"
          if [ -z "${{ secrets.SECRET_KEY }}" ]; then
            echo "Error: SECRET_KEY is empty or unset"
            exit 1
          fi
          kubectl create secret generic app-secrets --from-literal=secret-key=${{ secrets.SECRET_KEY }} -n default --dry-run=client -o yaml | kubectl apply -f --request-timeout=60s
      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.0'
      - name: Install AWS Load Balancer Controller
        continue-on-error: true
        run: |
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          ALB_CONTROLLER_ROLE_ARN=$(cat ../infra/terraform_outputs.json | jq -r '.aws_load_balancer_controller_role_arn.value')
          helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
            --namespace kube-system \
            --set clusterName=3tier-eks \
            --set serviceAccount.create=true \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"=$ALB_CONTROLLER_ROLE_ARN \
            --wait --timeout 10m
      - name: Deploy to EKS with Helm
        if: github.event.inputs.destroy != 'true' && success()
        continue-on-error: true
        run: |
          helm package helm/fastapi-app
          mv fastapi-app-*.tgz fastapi-app.tgz
          helm upgrade --install fastapi-app ./fastapi-app.tgz --namespace default --set image.repository=docker.io/${{ secrets.DOCKERHUB_USERNAME }}/3tier-app --set image.tag=${{ github.sha }} --timeout 15m
      - name: Debug Ingress and ALB
        if: github.event.inputs.destroy != 'true' && success()
        continue-on-error: true
        run: |
          echo "Listing all ingresses:"
          kubectl get ingress -n default -o yaml || true
          echo "Describing ingress (if any):"
          kubectl describe ingress -n default || true
          echo "Checking ALB controller pods:"
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -o wide || true
      - name: Get ALB URL
        if: github.event.inputs.destroy != 'true' && success()
        id: alb_url
        continue-on-error: true
        run: |
          for i in {1..15}; do
            ALB_URL=$(kubectl get ingress -n default -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}' --request-timeout=60s || true)
            if [ -n "$ALB_URL" ]; then
              echo "ALB_URL=$ALB_URL" >> $GITHUB_ENV
              echo "ALB URL: $ALB_URL"
              exit 0
            fi
            echo "Retrying ALB URL retrieval ($i/15)..."
            sleep 30
          done
          echo "Failed to retrieve ALB URL after 15 attempts"
          exit 1
      - name: Output ALB URL
        if: github.event.inputs.destroy != 'true' && success()
        run: |
          echo "Application is accessible at: http://${{ env.ALB_URL }}"