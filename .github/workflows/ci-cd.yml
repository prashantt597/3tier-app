name: CI/CD Pipeline
on:
  push:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      destroy:
        description: 'Trigger Terraform destroy'
        required: false
        default: 'false'
jobs:
  debug:
    runs-on: ubuntu-latest
    steps:
      - name: Debug trigger
        run: echo "Workflow triggered successfully"

  setup:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.7
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-south-1

  test:
    needs: setup
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          cd client
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip list
      - name: Check for dependency vulnerabilities
        run: |
          cd client
          pip install pip-audit
          pip-audit -r requirements.txt || true
      - name: Run unit tests with coverage
        run: |
          cd client
          python -m pytest -vv --cov=app --cov-report=xml tests/ || echo "Test errors: $(cat pytest.log)"

  scan:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Shallow clones should be disabled for better relevancy of analysis
      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      - name: Check and Create Docker Repository
        run: |
          REPO="${{ secrets.DOCKERHUB_USERNAME }}/3tier-app"
          if ! curl -s -f -u ${{ secrets.DOCKERHUB_USERNAME }}:${{ secrets.DOCKERHUB_TOKEN }} https://hub.docker.com/v2/repositories/$REPO/ > /dev/null 2>&1; then
            curl -X POST -u ${{ secrets.DOCKERHUB_USERNAME }}:${{ secrets.DOCKERHUB_TOKEN }} https://hub.docker.com/v2/repositories/ -d "{\"namespace\":\"${{ secrets.DOCKERHUB_USERNAME }}\",\"name\":\"3tier-app\",\"description\":\"Automated 3tier-app repository\"}"
            echo "Created Docker repository: $REPO"
          else
            echo "Docker repository $REPO already exists"
          fi
      - name: Build and push Docker image
        run: |
          cd client
          docker build -t ${{ secrets.DOCKERHUB_USERNAME }}/3tier-app:${{ github.sha }} .
          docker push ${{ secrets.DOCKERHUB_USERNAME }}/3tier-app:${{ github.sha }}
      - name: Verify Docker image availability
        run: |
          docker pull ${{ secrets.DOCKERHUB_USERNAME }}/3tier-app:${{ github.sha }} || { echo "Failed to pull image"; exit 1; }
      - name: Run Trivy scan
        uses: aquasecurity/trivy-action@master
        env:
          TRIVY_DISABLE_VEX_NOTICE: true
        with:
          image-ref: 'docker.io/${{ secrets.DOCKERHUB_USERNAME }}/3tier-app:${{ github.sha }}'
          format: 'table'
          exit-code: '1'
          ignore-unfixed: true
          vuln-type: 'os,library'
          severity: 'CRITICAL,HIGH'
          skip-version-check: true
      - name: Debug SonarCloud configuration
        run: |
          echo "SONAR_HOST_URL: https://sonarcloud.io"
          echo "SONAR_TOKEN is set: ${{ secrets.SONAR_TOKEN != '' }}"
      - name: SonarCloud Scan
        uses: SonarSource/sonarqube-scan-action@v5
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: https://sonarcloud.io
        with:
          projectBaseDir: client
          args: >
            -Dsonar.projectKey=prashantt597_3tier-app
            -Dsonar.organization=${{ secrets.SONAR_ORGANIZATION }} # Add this if you have a SonarCloud organization
            -Dsonar.sources=.
            -Dsonar.exclusions=tests/**,*/migrations/*
            -Dsonar.python.coverage.reportPaths=coverage.xml
      - name: Debug Docker container
        run: |
          docker run -d -p 8000:8000 --name test-container -e SECRET_KEY=${{ secrets.SECRET_KEY }} ${{ secrets.DOCKERHUB_USERNAME }}/3tier-app:${{ github.sha }}
          sleep 20
          docker ps -a
          docker logs test-container || true
          netstat -tuln | grep 8000 || true
      - name: Verify Docker image health
        continue-on-error: true
        run: |
          docker run -d -p 8000:8000 --name test-container -e SECRET_KEY=${{ secrets.SECRET_KEY }} ${{ secrets.DOCKERHUB_USERNAME }}/3tier-app:${{ github.sha }}
          sleep 20
          for i in {1..3}; do
            curl -f http://localhost:8000/health && exit 0
            echo "Retry $i: Health check failed, retrying..."
            sleep 5
          done
          echo "Health check failed after 3 retries"
          docker logs test-container
          exit 1

  deploy:
    needs: scan
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-south-1
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.7
      - name: Terraform Init (Local Backend)
        run: |
          cd infra
          terraform init
      - name: Terraform Apply S3 Resources
        run: |
          cd infra
          echo "Applying with account_id: ${{ secrets.AWS_ACCOUNT_ID }}"
          terraform apply -target=random_string.s3_prefix -target=aws_s3_bucket.terraform_state -target=aws_s3_bucket_versioning.terraform_state -target=aws_s3_bucket_public_access_block.terraform_state -target=aws_dynamodb_table.terraform_locks -auto-approve -var="region=ap-south-1" -var="account_id=${{ secrets.AWS_ACCOUNT_ID }}" -var="image_repository=docker.io/${{ secrets.DOCKERHUB_USERNAME }}/3tier-app" -var="image_tag=latest"
      - name: Terraform Init with S3 Backend
        run: |
          cd infra
          terraform init -backend-config="bucket=terraform-state-${{ steps.get_prefix.outputs.prefix }}-${{ secrets.AWS_ACCOUNT_ID }}" \
                         -backend-config="key=terraform.tfstate" \
                         -backend-config="region=ap-south-1" \
                         -backend-config="dynamodb_table=terraform-locks-${{ steps.get_prefix.outputs.prefix }}-${{ secrets.AWS_ACCOUNT_ID }}"
        id: init_with_backend
      - name: Get Random Prefix
        id: get_prefix
        run: |
          cd infra
          echo "Checking Terraform state..."
          terraform state list || echo "State list failed"
          PREFIX=$(terraform output -raw s3_prefix || echo "Output not found")
          echo "prefix=$PREFIX" >> $GITHUB_OUTPUT
          echo "Generated prefix: $PREFIX"
      - name: Debug Terraform configuration
        run: |
          cd infra
          terraform version
          terraform init -reconfigure
          terraform providers
          terraform plan -out=tfplan -var="vpc_cidr=10.0.0.0/16" -var="cluster_name=3tier-eks" -var="node_count=2" -var="region=ap-south-1" -var="account_id=${{ secrets.AWS_ACCOUNT_ID }}" -var="image_repository=docker.io/${{ secrets.DOCKERHUB_USERNAME }}/3tier-app" -var="image_tag=${{ github.sha }}"
          terraform show tfplan
          terraform output -json > terraform_outputs.json
      - name: Terraform Apply
        if: github.event.inputs.destroy != 'true'
        run: |
          cd infra
          terraform apply -auto-approve -var="vpc_cidr=10.0.0.0/16" -var="cluster_name=3tier-eks" -var="node_count=2" -var="region=ap-south-1" -var="account_id=${{ secrets.AWS_ACCOUNT_ID }}" -var="image_repository=docker.io/${{ secrets.DOCKERHUB_USERNAME }}/3tier-app" -var="image_tag=${{ github.sha }}"
      - name: Terraform Destroy
        if: github.event.inputs.destroy == 'true'
        run: |
          cd infra
          terraform destroy -auto-approve -var="vpc_cidr=10.0.0.0/16" -var="cluster_name=3tier-eks" -var="node_count=2" -var="region=ap-south-1" -var="account_id=${{ secrets.AWS_ACCOUNT_ID }}" -var="image_repository=docker.io/${{ secrets.DOCKERHUB_USERNAME }}/3tier-app" -var="image_tag=${{ github.sha }}"
      - name: Wait for and Recreate EKS Cluster if Needed
        if: github.event.inputs.destroy != 'true'
        continue-on-error: true
        run: |
          echo "Waiting for or recreating EKS cluster..."
          for i in {1..60}; do
            STATUS=$(aws eks --region ap-south-1 describe-cluster --name 3tier-eks --query 'cluster.status' --output text 2>/dev/null || echo "NOT_FOUND")
            if [ "$STATUS" = "ACTIVE" ]; then
              echo "Cluster is ACTIVE"
              exit 0
            elif [ "$STATUS" = "NOT_FOUND" ]; then
              echo "Cluster not found, triggering recreation ($i/60)..."
              cd infra
              terraform apply -auto-approve -var="vpc_cidr=10.0.0.0/16" -var="cluster_name=3tier-eks" -var="node_count=2" -var="region=ap-south-1" -var="account_id=${{ secrets.AWS_ACCOUNT_ID }}" -var="image_repository=docker.io/${{ secrets.DOCKERHUB_USERNAME }}/3tier-app" -var="image_tag=${{ github.sha }}"
              cd ..
            else
              echo "Cluster status: $STATUS, retrying ($i/60)..."
            fi
            sleep 30
          done
          echo "Cluster did not become ACTIVE after 30 minutes"
          exit 1
      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'
      - name: Debug kubectl configuration
        continue-on-error: true
        run: |
          aws eks --region ap-south-1 describe-cluster --name 3tier-eks --query 'cluster.{endpoint:endpoint, status:status, vpcId:vpcId, subnetIds:subnetIds, securityGroups:securityGroups}' --output json || true
          aws eks --region ap-south-1 update-kubeconfig --name 3tier-eks
          kubectl config view
          kubectl cluster-info || true
          kubectl get nodes -o wide || true
          echo "Testing EKS endpoint connectivity"
          EKS_ENDPOINT=$(aws eks describe-cluster --region ap-south-1 --name 3tier-eks --query 'cluster.endpoint' --output text)
          curl -k -v $EKS_ENDPOINT || true
          ping -c 3 $EKS_ENDPOINT || true
      - name: Configure kubectl
        run: |
          aws eks --region ap-south-1 update-kubeconfig --name 3tier-eks
      - name: Create Kubernetes Secret
        continue-on-error: true
        run: |
          echo "SECRET_KEY is set: ${{ secrets.SECRET_KEY != '' }}"
          if [ -z "${{ secrets.SECRET_KEY }}" ]; then
            echo "Error: SECRET_KEY is empty or unset"
            exit 1
          fi
          kubectl create secret generic app-secrets --from-literal=secret-key=${{ secrets.SECRET_KEY }} -n default --dry-run=client -o yaml | kubectl apply -f --request-timeout=60s
      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.0'
      - name: Install AWS Load Balancer Controller
        continue-on-error: true
        run: |
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          ALB_CONTROLLER_ROLE_ARN=$(cat ../infra/terraform_outputs.json | jq -r '.aws_load_balancer_controller_role_arn.value')
          helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
            --namespace kube-system \
            --set clusterName=3tier-eks \
            --set serviceAccount.create=true \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"=$ALB_CONTROLLER_ROLE_ARN \
            --wait --timeout 10m
      - name: Deploy to EKS with Helm
        if: github.event.inputs.destroy != 'true'
        continue-on-error: true
        run: |
          helm package helm/fastapi-app
          mv fastapi-app-*.tgz fastapi-app.tgz
          helm upgrade --install fastapi-app ./fastapi-app.tgz --namespace default --set image.repository=docker.io/${{ secrets.DOCKERHUB_USERNAME }}/3tier-app --set image.tag=${{ github.sha }} --timeout 15m
      - name: Debug Ingress and ALB
        if: github.event.inputs.destroy != 'true'
        continue-on-error: true
        run: |
          echo "Listing all ingresses:"
          kubectl get ingress -n default -o yaml || true
          echo "Describing ingress (if any):"
          kubectl describe ingress -n default || true
          echo "Checking ALB controller pods:"
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -o wide || true
      - name: Get ALB URL
        if: github.event.inputs.destroy != 'true'
        id: alb_url
        continue-on-error: true
        run: |
          for i in {1..15}; do
            ALB_URL=$(kubectl get ingress -n default -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}' --request-timeout=60s || true)
            if [ -n "$ALB_URL" ]; then
              echo "ALB_URL=$ALB_URL" >> $GITHUB_ENV
              echo "ALB URL: $ALB_URL"
              exit 0
            fi
            echo "Retrying ALB URL retrieval ($i/15)..."
            sleep 30
          done
          echo "Failed to retrieve ALB URL after 15 attempts"
          exit 1
      - name: Output ALB URL
        if: github.event.inputs.destroy != 'true'
        run: |
          echo "Application is accessible at: http://${{ env.ALB_URL }}"